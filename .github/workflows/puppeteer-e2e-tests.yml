name: Puppeteer E2E Tests

on:
  pull_request:
    types: [labeled]
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - deep
          - ui
          - ssrf

permissions: {}  # Minimal top-level for OSSF Scorecard

jobs:
  puppeteer-tests:
    name: Puppeteer E2E Tests
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'workflow_dispatch' ||
      github.event.label.name == 'test:puppeteer' ||
      github.event.label.name == 'test:e2e'
    permissions:
      contents: read
      pull-requests: write
      issues: write
    env:
      LABEL_NAME: ${{ github.event.label.name || 'manual' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3 # v6.0.0

      - name: Setup Python
        uses: actions/setup-python@83679a892e2d95755f2dac6acb0bfd1e9ac5d548 # v6
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'pyproject.toml'

      - name: Setup Node.js
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: 'tests/puppeteer/package-lock.json'

      - name: Install Python dependencies
        run: pip install -e .

      - name: Install Puppeteer dependencies
        working-directory: tests/puppeteer
        run: |
          npm ci
          npx puppeteer browsers install chrome

      - name: Create test configuration
        run: |
          # Create config directory if needed
          mkdir -p ~/.config/local-deep-research

          # Create settings with OpenRouter + Gemini Flash 2.0
          cat > ~/.config/local-deep-research/settings.toml << 'EOF'
          [llm]
          provider = "openrouter"
          model = "google/gemini-2.0-flash-001"

          [search]
          tool = "serper"
          iterations = 1
          questions_per_iteration = 2

          [general]
          report_type = "quick"
          EOF

      - name: Start LDR server
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          SERPER_API_KEY: ${{ secrets.SERPER_API_KEY }}
        run: |
          # Start server in background
          cd src
          nohup python -m local_deep_research.web.app > /tmp/ldr_server.log 2>&1 &

          # Wait for server to be ready
          echo "Waiting for server to start..."
          for i in {1..30}; do
            if curl -s http://localhost:5000/auth/login > /dev/null 2>&1; then
              echo "Server is ready!"
              break
            fi
            echo "Waiting... ($i/30)"
            sleep 2
          done

          # Check if server started successfully
          if ! curl -s http://localhost:5000/auth/login > /dev/null 2>&1; then
            echo "Server failed to start. Logs:"
            cat /tmp/ldr_server.log
            exit 1
          fi

      - name: Run Puppeteer tests
        working-directory: tests/puppeteer
        env:
          TEST_URL: http://localhost:5000
          HEADLESS: true
        run: |
          TEST_SUITE="${{ github.event.inputs.test_suite || 'all' }}"

          # Capture output to file while also showing in console
          case "${TEST_SUITE}" in
            deep)
              npm run test:deep 2>&1 | tee test-output.log
              ;;
            ui)
              npm run test:ui 2>&1 | tee test-output.log
              ;;
            ssrf)
              npm run test:ssrf 2>&1 | tee test-output.log
              ;;
            all|*)
              # Run all test suites with CI script
              npm run test:ci 2>&1 | tee test-output.log
              ;;
          esac

          # Save exit code
          echo "TEST_EXIT_CODE=${PIPESTATUS[0]}" >> "$GITHUB_ENV"

      - name: Extract test summary
        if: always()
        working-directory: tests/puppeteer
        run: |
          # Extract pass/fail summary from mocha output
          if [ -f test-output.log ]; then
            # Get the summary line (e.g., "26 passing (3m)")
            PASSING=$(grep -oP '\d+ passing' test-output.log | tail -1 || echo "0 passing")
            FAILING=$(grep -oP '\d+ failing' test-output.log | tail -1 || echo "0 failing")

            echo "TESTS_PASSING=${PASSING}" >> "$GITHUB_ENV"
            echo "TESTS_FAILING=${FAILING}" >> "$GITHUB_ENV"

            # Extract key test results (collection creation, subscription, research)
            echo "## Key Test Results" > test-summary.md
            echo "" >> test-summary.md

            # Check for collection creation
            if grep -q "Collection found in dropdown: true" test-output.log; then
              echo "- ‚úÖ Collection creation: **Success**" >> test-summary.md
            elif grep -q "Collection found in dropdown: false" test-output.log; then
              echo "- ‚ùå Collection creation: **Failed**" >> test-summary.md
            fi

            # Check for subscription creation
            if grep -q "Subscription name found: true" test-output.log; then
              echo "- ‚úÖ Subscription creation: **Success**" >> test-summary.md
            elif grep -q "Subscription name found: false" test-output.log; then
              echo "- ‚ùå Subscription creation: **Failed**" >> test-summary.md
            fi

            # Check for research completion
            if grep -q "Research completed: true" test-output.log; then
              RESEARCH_TIME=$(grep -oP "Research completed: true \(took \K\d+(?=s\))" test-output.log || echo "?")
              echo "- ‚úÖ Research workflow: **Completed** (${RESEARCH_TIME}s)" >> test-summary.md
            elif grep -q "Research completed: false" test-output.log; then
              echo "- ‚ö†Ô∏è Research workflow: **Timed out**" >> test-summary.md
            fi

            # Check for settings persistence
            if grep -q "After reload value:" test-output.log; then
              echo "- ‚úÖ Settings persistence: **Working**" >> test-summary.md
            fi

            # Add screenshot count
            SCREENSHOT_COUNT=$(find screenshots -name "*.png" 2>/dev/null | wc -l || echo "0")
            echo "" >> test-summary.md
            echo "**Screenshots captured:** ${SCREENSHOT_COUNT}" >> test-summary.md
          fi

      - name: Upload server logs on failure
        if: failure()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
        with:
          name: server-logs
          path: /tmp/ldr_server.log
          retention-days: 7

      - name: Post test results
        if: always() && github.event_name == 'pull_request'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          {
            echo "## üß™ Puppeteer E2E Test Results"
            echo ""
            if [ "${{ env.TEST_EXIT_CODE }}" == "0" ]; then
              echo "### ‚úÖ Tests Passed!"
            else
              echo "### ‚ùå Tests Failed"
            fi
            echo ""
            echo "**Summary:** ${{ env.TESTS_PASSING }}, ${{ env.TESTS_FAILING }}"
            echo ""

            # Include key test results
            if [ -f tests/puppeteer/test-summary.md ]; then
              cat tests/puppeteer/test-summary.md
              echo ""
            fi

            echo "---"
            echo ""
            echo "<details>"
            echo "<summary>üìã Full Test Output (click to expand)</summary>"
            echo ""
            echo '```'
            # Get last 100 lines of test output (most relevant)
            if [ -f tests/puppeteer/test-output.log ]; then
              tail -100 tests/puppeteer/test-output.log
            fi
            echo '```'
            echo "</details>"
            echo ""
            echo "---"
            echo ""
            echo "**Configuration:**"
            echo "- Model: \`google/gemini-2.0-flash-001\` (via OpenRouter)"
            echo "- Search: \`serper\`"
            echo "- Test suite: \`${{ github.event.inputs.test_suite || 'all' }}\`"
            echo ""
            echo "_Triggered by label: \`${{ env.LABEL_NAME }}\`_"
          } > comment.md

          gh pr comment ${{ github.event.pull_request.number }} --repo ${{ github.repository }} -F comment.md

      - name: Remove label for re-triggering
        if: always() && github.event_name == 'pull_request'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh issue edit ${{ github.event.pull_request.number }} --remove-label "$LABEL_NAME" 2>/dev/null || true

      - name: Cleanup
        if: always()
        run: |
          # Kill server
          pkill -f 'python -m local_deep_research.web.app' || true
          rm -f comment.md
